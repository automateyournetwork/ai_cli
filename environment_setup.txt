#from the IOS XE CLI drop into guestshell
guestshell

#stop the ollama service if its running (after install it will setup as a service use this command top stop it)
sudo systemctl stop ollama

#setup your local environment for proxy
export http_proxy=http://proxy.esl.cisco.com:8080/
export https_proxy=http://proxy.esl.cisco.com:8080/
export HTTP_PROXY=http://proxy.esl.cisco.com:8080/
export HTTPS_PROXY=http://proxy.esl.cisco.com:8080/

#setup your local models folder somewhere in guestshell with lots of disk
export OLLAMA_MODELS=/data/ollama/models/

#setup folder and permissions for ollama temp files
sudo mkdir -p /data/ollama/tmp
sudo chown -R $USER:$USER /data/ollama/tmp
sudo chmod -R 755 /data/ollama/tmp
export OLLAMA_TMPDIR=/data/ollama/tmp

#start the ollama server
ollama serve

# from another terminal session download models like phi3 or tinyllama or llama3 
ollama pull <model name>

# test / list models
ollama list